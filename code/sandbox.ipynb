{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping tool (NB)\n",
    "\n",
    "In this document lot of nice thing will happen :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "import openai\n",
    "import MACHINE_CONFIG\n",
    "import newspaper\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.parse import urlparse\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scraper\n",
    "import os\n",
    "CONFIG_PATH = r\"C:\\Users\\andra\\OneDrive\\Dokumentumok\\Munka\\WeSpeakAi\\GoodPeople\\site-config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.site_selector(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Search for the page articles\n",
    "\n",
    "Our first task is to collect every article from the site, but only the articles that we want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(base_url, article_rule, max_url = 1000, output_path=None, verbal=False):\n",
    "    # Base URL of the website to crawl\n",
    "    visited_urls = set()\n",
    "    articles = set()\n",
    "    urls_to_visit = deque([base_url])\n",
    "\n",
    "    # Function to check if a URL belongs to the base domain\n",
    "    def is_same_domain(url, base_domain):\n",
    "        return urlparse(url).netloc == base_domain\n",
    "\n",
    "    # Starting the crawl\n",
    "    while urls_to_visit and len(articles)<max_url:\n",
    "        current_url = urls_to_visit.popleft()\n",
    "        \n",
    "        # Skip if already visited\n",
    "        if current_url in visited_urls:\n",
    "            continue\n",
    "        \n",
    "        # Mark as visited\n",
    "        visited_urls.add(current_url)\n",
    "        if verbal: print(f\"Visiting: {current_url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                links = soup.find_all('a')\n",
    "                filtered_links = [s.get('href') for s in links if (not s.get('href') is None and (s.get('href').startswith('/') or s.get('href').startswith(base_url)))]\n",
    "\n",
    "                for link in filtered_links:\n",
    "                    if link:\n",
    "                        full_url = urljoin(base_url, link)\n",
    "                        if is_same_domain(full_url, urlparse(base_url).netloc):\n",
    "                            urls_to_visit.append(full_url)\n",
    "                        if full_url.startswith(str(base_url+article_rule)): \n",
    "                            articles.add(full_url)\n",
    "                            print(f\"Count: [{len(articles)} of {max_url}]\", flush=True)\n",
    "            else:\n",
    "                print(f\"Failed to fetch {current_url}. Status code: {response.status_code}\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {current_url}: {e}\")\n",
    "    if output_path:\n",
    "        with open(output_path, 'w') as file:\n",
    "            # Iterate through the list of URLs\n",
    "            for url in articles:\n",
    "                # Write each URL to the file followed by a newline character\n",
    "                file.write(url + '\\n')\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def has_common_element(list1, list2):\n",
    "    # Convert both lists to sets\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    \n",
    "    # Check if there is any intersection between the sets\n",
    "    return not set1.isdisjoint(set2)\n",
    "\n",
    "def extract_and_count_urls(urls, lenght_of_posible_article):\n",
    "    # Dictionary to hold the occurrences of each segment\n",
    "    segment_counts = defaultdict(int)\n",
    "    \n",
    "    # List to store lists of segments for each URL\n",
    "    all_segments = []\n",
    "    articles = []\n",
    "    \n",
    "    for url in urls:\n",
    "        # Parse the URL and extract the path\n",
    "        parsed_url = urlparse(url)\n",
    "        path = parsed_url.path\n",
    "        segments = []\n",
    "        # Split the path into segments and filter out empty segments\n",
    "        for segment in path.split('/'):\n",
    "            if segment:\n",
    "                if len(segment) <= lenght_of_posible_article:\n",
    "                    segments.append(segment)\n",
    "                else:\n",
    "                    segments.append(\"possible_article\")\n",
    "                    articles.append(parsed_url)\n",
    "        # segments = [segment if len(segment) <= lenght_of_posible_article else \"possible_article\" for segment in path.split('/') if segment]\n",
    "        \n",
    "        # Append the list of segments for this URL to all_segments\n",
    "        all_segments.append(segments)\n",
    "        \n",
    "        # Count each segment\n",
    "        for segment in segments:\n",
    "            segment_counts[segment] += 1\n",
    "    \n",
    "    # Convert the dictionary to a list of lists format\n",
    "    segment_occurrences = [[segment, count] for segment, count in segment_counts.items()]\n",
    "    \n",
    "    return all_segments, segment_occurrences, articles\n",
    "\n",
    "def visualize_segment_occurrences(segment_occurrences):\n",
    "    # Sort the segments by their count in descending order\n",
    "    segment_occurrences.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Unzip the list of lists into two lists\n",
    "    segments, counts = zip(*segment_occurrences)\n",
    "    \n",
    "    # Create a bar plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(segments, counts, color='skyblue')\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title('Occurrences of URL Segments')\n",
    "    plt.xlabel('Segments')\n",
    "    plt.ylabel('Occurrences')\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def short_urls(urls, categories, cfp=False,  min_article_lenght=12):\n",
    "    \"\"\"\n",
    "    cfp: Chategory is first in path\n",
    "    \"\"\"\n",
    "    articles = []\n",
    "    for url in urls:\n",
    "        # Parse the URL and extract the path\n",
    "        parsed_url = urlparse(url)\n",
    "        path = parsed_url.path\n",
    "        segments = [segment for segment in path.split('/') if segment]\n",
    "        has_chategory = (segments[0] in categories) if cfp and len(segments)>0 else has_common_element(categories, segments)\n",
    "        filtered_segments = [element for element in segments if element not in categories]\n",
    "        has_posible_article_lenght = any([len(element) >= min_article_lenght for element in filtered_segments if len(element) >= min_article_lenght])\n",
    "        if has_chategory and has_posible_article_lenght:\n",
    "            articles.append(parsed_url)\n",
    "    return articles\n",
    "     \n",
    "categories = [\"environment\", \"society\", \"lifestyle\", \"science\", \"economics\", \"perspective\", \"opinion\"]\n",
    "\n",
    "#get_articles(\"https://reasonstobecheerful.world/\", \"\", 100, r\"test.txt\")\n",
    "# # Example usage:\n",
    "with open(r\"test.txt\", \"r\") as file:\n",
    "    urls = file.readlines()\n",
    "\n",
    "# Extract segments and their occurrences\n",
    "segments, occurrences, articles = extract_and_count_urls(urls, 10)\n",
    "\n",
    "# articles = short_urls(urls, categories, True, 12)\n",
    "\n",
    "# Print extracted segments and occurrences\n",
    "print(\"Segments extracted from each URL:\", segments)\n",
    "print(\"Occurrences of each segment:\", occurrences)\n",
    "print(f\"Articles: {len(articles)}\")\n",
    "for article in articles:\n",
    "    print(article.netloc + article.path)\n",
    "\n",
    "# Visualize the occurrences\n",
    "visualize_segment_occurrences(occurrences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {[\n",
    "            {\n",
    "              \"name\": \":authority\",\n",
    "              \"value\": \"backend.thehappybroadcast.com\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \":method\",\n",
    "              \"value\": \"POST\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \":path\",\n",
    "              \"value\": \"/graphql\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \":scheme\",\n",
    "              \"value\": \"https\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"accept\",\n",
    "              \"value\": \"application/json\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"accept-encoding\",\n",
    "              \"value\": \"gzip, deflate, br, zstd\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"accept-language\",\n",
    "              \"value\": \"en-GB,en;q=0.9,en-US;q=0.8,hu;q=0.7,es;q=0.6\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"authorization\",\n",
    "              \"value\": \"Bearer thb2021tokenW3b\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"content-length\",\n",
    "              \"value\": \"377\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"content-type\",\n",
    "              \"value\": \"application/json\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"dnt\",\n",
    "              \"value\": \"1\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"origin\",\n",
    "              \"value\": \"https://www.thehappybroadcast.com\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"priority\",\n",
    "              \"value\": \"u=1, i\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"referer\",\n",
    "              \"value\": \"https://www.thehappybroadcast.com/\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"sec-ch-ua\",\n",
    "              \"value\": \"\\\"Not/A)Brand\\\";v=\\\"8\\\", \\\"Chromium\\\";v=\\\"126\\\", \\\"Microsoft Edge\\\";v=\\\"126\\\"\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"sec-ch-ua-mobile\",\n",
    "              \"value\": \"?0\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"sec-ch-ua-platform\",\n",
    "              \"value\": \"\\\"Windows\\\"\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"sec-fetch-dest\",\n",
    "              \"value\": \"empty\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"sec-fetch-mode\",\n",
    "              \"value\": \"cors\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"sec-fetch-site\",\n",
    "              \"value\": \"same-site\"\n",
    "            },\n",
    "            {\n",
    "              \"name\": \"user-agent\",\n",
    "              \"value\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36 Edg/126.0.0.0\"\n",
    "            }\n",
    "          ]}\n",
    "\n",
    "url_search='https://backend.thehappybroadcast.com/graphql'\n",
    "infos = {\n",
    "    \"queryString\": [],\n",
    "    \"cookies\": [],\n",
    "    \"headersSize\": -1,\n",
    "    \"bodySize\": 377,\n",
    "    \"postData\": {\n",
    "        \"mimeType\": \"application/json\",\n",
    "        \"text\": \"{\\\"query\\\":\\\"\\\\nquery getArchive($limit: Int, $start: Int) {\\\\n  archive {\\\\n    seo {\\\\n      title\\\\n      description\\\\n    }\\\\n  }\\\\n  posts(limit: $limit, start: $start, sort: \\\\\\\"date:desc\\\\\\\") {\\\\n    title\\\\n    slug\\\\n    date\\\\n    color\\\\n    caption\\\\n    image {\\\\n      url\\\\n      width\\\\n      height\\\\n    }\\\\n  } \\\\n}\\\\n\\\",\\\"variables\\\":{\\\"limit\\\":50,\\\"start\\\":100,\\\"dateGt\\\":null,\\\"dateLt\\\":null}}\"\n",
    "    }\n",
    "}\n",
    "\n",
    "data={\n",
    "    \"mimeType\": \"application/json\",\n",
    "    \"text\": \"{\\\"query\\\":\\\"\\\\nquery getArchive($limit: Int, $start: Int) {\\\\n  archive {\\\\n    seo {\\\\n      title\\\\n      description\\\\n    }\\\\n  }\\\\n  posts(limit: $limit, start: $start, sort: \\\\\\\"date:desc\\\\\\\") {\\\\n    title\\\\n    slug\\\\n    date\\\\n    color\\\\n    caption\\\\n    image {\\\\n      url\\\\n      width\\\\n      height\\\\n    }\\\\n  } \\\\n}\\\\n\\\",\\\"variables\\\":{\\\"limit\\\":50,\\\"start\\\":100,\\\"dateGt\\\":null,\\\"dateLt\\\":null}}\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=requests.session()\n",
    "\n",
    "req=s.post(\n",
    "    url=url_search,\n",
    "    headers=headers,\n",
    "    json=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\")\n",
    "model = AutoModel.from_pretrained(\"SZTAKI-HLT/hubert-base-cc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Bizonyos életkörülmények között egy nagyobb lakás vagy autó megvásárlása nem várathat magára, még akkor sem, ha ehhez viszonylag magas kamatozású hitelre is szükségünk van. A gazdaság ciklikussága miatt azonban viszonylag rövid távon előállhat olyan helyzet, melyben már a korábbinál sokkal kedvezőbb kamatok mellett kaphatunk kölcsönt, ezért célszerű a régi hitelt lecserélni egy új, kedvezőbb konstrukcióra. Mivel napjainkban ehhez hasonló folyamatok zajlanak a hitelpiacon, az MBH Bank szakértői összefoglalták, mikor érdemes kiváltanunk régi hiteleinket.**\n",
      "\n",
      "Hitelfelvétel szempontjából most, 2024 májusában kedvező a helyzet, hiszen a gazdasági élet helyre állásával, a pénzügyi válság lecsengésével azt látjuk, hogy lassan egy éve csökkennek a hitelkamatok. Míg 2023. elején a lakáshitel kamatok 8-9% körül mozogtak, azok mára 6-7% körüli szintre mérséklődtek. De hasonló kamatesést figyelhettünk meg ezen távon  a fogyasztási hitelek esetében is. Igaz, a kamatszint még mindig nem tért vissza a 2022-ben látott 4-5%-os mértékre a jelzáloghiteleknél, személyi kölcsönök esetében pedig szintén nem 8% a legkedvezőbb elérhető kamat, de a trend jól láthatóan csökkenő; pénzügyi elemzők is hosszú távon további mérséklődést prognosztizálnak a hozamszintekben. Mindez azt jelenti, hogy jó eséllyel láthatunk majd a mainál is vonzóbb banki hitelkondíciókat.\n",
      "\n",
      "**Tényleg megéri?**\n",
      "\n",
      "Mindezek tükrében joggal merülhet fel a kérdés minden olyan ügyfélben, akinek úgy hozta az élet, hogy magas kamat mellett vett fel lakáshitelt vagy fedezetlen személyi kölcsönt, hogy mikor érdemes egy új hitellel kiváltani a meglévőt. A hitelkiváltás valóban segítség lehet a havi költségvetésünk javítása szempontjából, hiszen a törlesztőrészlet csökkenésén megtakarított összeget számos egyéb hasznos dologra is felhasználhatjuk. De mit is kell számba vennünk, amikor egy ilyen döntést szeretnénk meghozni? Mennyi lesz a havi megtakarítás? Milyen költségek merülnek fel a tranzakció során? Mennyi ideig fog ez tartani?\n",
      "\n",
      "A döntésünk megalapozásához elsősorban számolnunk kell. Kézenfekvő megoldás, hogy első lépésben felkeressük azt a bankfiókot, ahol anno a hitelt felvettük, és a hiteltanácsadó segítségét kérjük, vagy tájékozódjunk a jelenleg elérhető kamatokról az interneten elérhető információk segítségével. A jelenleg igényelhető hitelek kondíciói alapján tájékoztatást kaphatunk, hogy milyen induló díjakkal kell számolnunk, milyen akciók vannak, valamint végső soron mekkorák lennének a kiváltó hitel havi törlesztőrészletei a mai kamatok mellett.\n",
      "\n",
      "A még hátralévő futamidőhöz képest opcionálisan hosszabb futamidőt is választhatunk, aminek köszönhetően tovább redukálható a havi teher, de ezzel több hitelkamat fizetését is vállalnunk kell. Tehát ha például 13 év van már csak hátra, akkor a hitelkiváltás során választhatunk 13 éves futamidőt, de akár 15 vagy 20 éveset is. Fontos tudni, hogy mi a minimális hossza a hitelnek, mert ha annál kevesebb van hátra, akkor ugyanolyan lejáratot nem választhatunk. Természetesen személyi kölcsönök esetében jelentősen alacsonyabb a futamidő és a hitelösszegek is, de így is komoly összegeket takaríthatunk meg, vagy akár ugyanakkora törlesztőrészlet mellett extra felhasználható összeghez juthatunk.\n",
      "\n",
      "**Figyeljünk a költségekre!**\n",
      "\n",
      "A döntésünknél fontos mérlegelendő tényező, hogy a meglévő jelzáloghitelünktől milyen módon tudunk megválni. Ha az MBH Banknál vagy jogelőd intézményénél három éven belül vettük fel a kölcsönt, a kapott induló díjkedvezményeket (például értékbecslési díj, közjegyzői díj, földhivatali díj visszatérítés) vissza kell fizetnünk. Szerencsére bankon belüli hitelkiváltásnál jellemzően a hitelt nyújtók nem számítják fel az előtöresztési díjat, de ha más bankhoz fordulnánk, úgy a még fennálló tartozás 1-1,5%-át is ki fogják számlázni nekünk. A lakáshitel jelzálogjoggal fedezett termék, így az ennek biztosítására bejegyzett jog törlésének földhivatali eljárási illetékét is le kell még rónunk (jelenleg 6.600 Ft). Ezen felül kell figyelembe venni a kiváltó hitelnél felmerülő díjakat és költségeket.\n",
      "\n",
      "Személyi kölcsön esetében az MBH Bankban nincsenek induló díjak sem új igénylés, sem kiváltás esetén, így a döntést leginkább az elérhető kamat mértéke befolyásolhatja.\n",
      "\n",
      "**Így néz ki a gyakorlatban**\n",
      "\n",
      "Az elérhető megtakarítás és a felmerülő ráfordítások számbavételéhez nyújt segítséget az alábbi példa. Az eredeti hitel összeg 15.000.000 Ft, futamidő 20 év, a jelenlegi kamat 8,74%. Az új hitel 6,79%-os kamaton érhető el, a futamidő megegyezik a most hátralévővel, azaz 18 év. Figyelembe vettük, hogy a tartozás időközben már 610.000 Ft-tal csökkent:\n",
      "\n",
      "[![Image 1](https://monitorblog.hu/wp-content/uploads/2024/05/tablazat_hitel-1-1024x286.png)](https://monitorblog.hu/wp-content/uploads/2024/05/tablazat_hitel-1.png)\n",
      "\n",
      "Ebből megállapítható, hogy közel 2%-kal alacsonyabb kamat mellett már érdemi törlesztőrészlet csökkenés érhető el, hiszen havonta közel 17.000 Ft-tal több marad a pénztárcánkban. A régi hitel lezárási, míg az új hitel igénybevételi díjai azonban magasak, előre 213.300 Ft-ot kell kifizetnünk a cseréért. Csökkenthetjük a ráfordításainkat, ha megvárjuk azt az időszakot, amikor már nem merül fel a jelenleg futó hitelnek az induló átvállalt díj visszatérítése, ez bankonként jellemzően 3-4 év között mozog. A példánál maradva, a második évtől hitelfelvevői aspektusból nézve már pozitív egyenlege lesz a tranzakciónak.\n",
      "\n",
      "Folyamati szempontból a taglalt banki tranzakció a legegyszerűbbek közé tartozik, tehát a hitelkérelem benyújtásától a hitel folyósításáig rövid, nagyjából 3-4 hetes átfutási idővel számolhatunk – míg ugyanez személyi kölcsön esetében még rövidebb, általában maximum 1-2 nap.\n",
      "\n",
      "Viszlay Zoltán,  \n",
      "Mag Mihály\n",
      "\n",
      "_A fenti cikk marketingközleménynek minősül, mellyel kapcsolatban ajánljuk olvasóink figyelmébe az_ [**_itt elérhető jogi tájékoztatót._**](https://monitorblog.hu/jogi-tajekoztato-marketingkozlemenyek/)\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:\\Users\\andra\\OneDrive\\Dokumentumok\\Munka\\WeSpeakAi\\GoodPeople\\data\\mbh\\6845f0f8-32e9-11ef-9b29-8272bb82d234.md\"\n",
    "with open(path, \"r\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(type(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keyphrasetransformer import KeyPhraseTransformer\n",
    "kp = KeyPhraseTransformer()\n",
    "kp.get_key_phrases(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.0 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\andra\\AppData\\Local\\Temp\\ipykernel_29172\\2356465878.py\", line 1, in <module>\n",
      "    from keybert import KeyBERT\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\keybert\\__init__.py\", line 3, in <module>\n",
      "    from keybert._llm import KeyLLM\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\keybert\\_llm.py\", line 4, in <module>\n",
      "    from sentence_transformers import util\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\sentence_transformers\\__init__.py\", line 15, in <module>\n",
      "    from sentence_transformers.trainer import SentenceTransformerTrainer\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\sentence_transformers\\trainer.py\", line 10, in <module>\n",
      "    from transformers import EvalPrediction, PreTrainedTokenizerBase, Trainer, TrainerCallback\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1525, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1535, in _get_module\n",
      "    return importlib.import_module(\".\" + module_name, self.__name__)\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\transformers\\trainer.py\", line 71, in <module>\n",
      "    from .optimization import Adafactor, get_scheduler\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\transformers\\optimization.py\", line 27, in <module>\n",
      "    from .trainer_pt_utils import LayerWiseDummyOptimizer, LayerWiseDummyScheduler\n",
      "  File \"c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\transformers\\trainer_pt_utils.py\", line 235, in <module>\n",
      "    device: Optional[torch.device] = torch.device(\"cuda\"),\n",
      "c:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\transformers\\trainer_pt_utils.py:235: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: Optional[torch.device] = torch.device(\"cuda\"),\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m     25\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is a sample text to extract keywords. KeyBERT is a great tool for keyword extraction tasks.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 26\u001b[0m keywords \u001b[38;5;241m=\u001b[39m \u001b[43mextract_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtracted Keywords:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m keywords:\n",
      "Cell \u001b[1;32mIn[8], line 20\u001b[0m, in \u001b[0;36mextract_keywords\u001b[1;34m(text, num_keywords)\u001b[0m\n\u001b[0;32m     17\u001b[0m ke \u001b[38;5;241m=\u001b[39m KeyBERT()\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Extract keywords\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m keywords \u001b[38;5;241m=\u001b[39m \u001b[43mke\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_keywords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keywords\n",
      "File \u001b[1;32mc:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\keybert\\_model.py:195\u001b[0m, in \u001b[0;36mKeyBERT.extract_keywords\u001b[1;34m(self, docs, candidates, keyphrase_ngram_range, stop_words, top_n, min_df, use_maxsum, use_mmr, diversity, nr_candidates, vectorizer, highlight, seed_keywords, doc_embeddings, word_embeddings, threshold)\u001b[0m\n\u001b[0;32m    193\u001b[0m \u001b[38;5;66;03m# Extract embeddings\u001b[39;00m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m doc_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 195\u001b[0m     doc_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    197\u001b[0m     word_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed(words)\n",
      "File \u001b[1;32mc:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\keybert\\backend\\_sentencetransformers.py:67\u001b[0m, in \u001b[0;36mSentenceTransformerBackend.embed\u001b[1;34m(self, documents, verbose)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Embed a list of n documents/words into an n-dimensional\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03mmatrix of embeddings\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;124;03m    that each have an embeddings size of `m`\u001b[39;00m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_progress_bar\u001b[39m\u001b[38;5;124m\"\u001b[39m: verbose})\n\u001b[1;32m---> 67\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_model\u001b[38;5;241m.\u001b[39mencode(documents, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_kwargs)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[1;32mc:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:568\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[1;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[0;32m    566\u001b[0m             all_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([emb\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m all_embeddings])\n\u001b[0;32m    567\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 568\u001b[0m             all_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([emb\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m all_embeddings])\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(all_embeddings, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    570\u001b[0m     all_embeddings \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mfrom_numpy(embedding) \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m all_embeddings]\n",
      "File \u001b[1;32mc:\\Users\\andra\\miniconda3\\envs\\wsa\\lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:568\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    566\u001b[0m             all_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([emb\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m all_embeddings])\n\u001b[0;32m    567\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 568\u001b[0m             all_embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray([\u001b[43memb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m emb \u001b[38;5;129;01min\u001b[39;00m all_embeddings])\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(all_embeddings, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m    570\u001b[0m     all_embeddings \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mfrom_numpy(embedding) \u001b[38;5;28;01mfor\u001b[39;00m embedding \u001b[38;5;129;01min\u001b[39;00m all_embeddings]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "\n",
    "# Define a function to extract keywords\n",
    "def extract_keywords(text, num_keywords=5):\n",
    "  \"\"\"\n",
    "  Extracts keywords from a given text using KeyBERT.\n",
    "\n",
    "  Args:\n",
    "      text: The text to extract keywords from.\n",
    "      num_keywords: The desired number of keywords to extract (default: 5).\n",
    "\n",
    "  Returns:\n",
    "      A list of the top keywords extracted from the text.\n",
    "  \"\"\"\n",
    "\n",
    "  # Create a KeyBERT model\n",
    "  ke = KeyBERT(model=)\n",
    "\n",
    "  # Extract keywords\n",
    "  keywords = ke.extract_keywords(text, top_n=num_keywords)\n",
    "\n",
    "  return keywords\n",
    "\n",
    "# Example usage\n",
    "text = \"This is a sample text to extract keywords. KeyBERT is a great tool for keyword extraction tasks.\"\n",
    "keywords = extract_keywords(text)\n",
    "\n",
    "print(\"Extracted Keywords:\")\n",
    "for keyword in keywords:\n",
    "  print(keyword)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraping with JINA-AI-READER\n",
    "\n",
    "*source:* [Jina](https://jina.ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jina_reader(url, save=False, output=None):\n",
    "    \"\"\"\n",
    "        The response from JINA has the following structure:\n",
    "        {\n",
    "            satus: 200\n",
    "            \"data\": {\n",
    "                \"title\": The title of the article,\n",
    "                \"url\": URL of the article\n",
    "                \"content\": The article text\n",
    "                \"publishedTime\": The publiseh time in '2024-06-17T10:34:05+00:00' format\n",
    "            }        \n",
    "        }\n",
    "\n",
    "    \"\"\"\n",
    "    jina = 'https://r.jina.ai/'\n",
    "    headers = {\n",
    "    \"Authorization\": MACHINE_CONFIG.JINA_KEY,\n",
    "    \"Accept\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(jina+url, headers=headers)\n",
    "    response_text = response.text\n",
    "    # Checking the status of the request\n",
    "    if response.status_code == 200:\n",
    "        response_body = response.content.decode('utf-8')\n",
    "        response_body = json.loads(response_body)[\"data\"]\n",
    "        print(response_body)\n",
    "        title = response_body[\"title\"].strip().replace(\" \", \"_\")\n",
    "        text = response_body[\"content\"]\n",
    "    else:\n",
    "        print(f'Failed to fetch data. Status code: {response.status_code}')\n",
    "    if save:\n",
    "        if  output is None: \n",
    "            output=str(title+\".md\")\n",
    "        with open(output, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text)\n",
    "    return response_body\n",
    "    \n",
    "def find_images_in_md_text(text):\n",
    "    \"\"\"\n",
    "    # Function to find all image links in a Markdown file\n",
    "    \"\"\"\n",
    "    image_pattern = r'!\\[(.*?)\\]\\((.*?)\\)'\n",
    "    matches = re.findall(image_pattern, text)\n",
    "\n",
    "    return matches\n",
    "\n",
    "def find_images_on_page(url):\n",
    "    article = newspaper.Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article.images\n",
    "\n",
    "def construct_article(url, save=False, output=None):\n",
    "    data = jina_reader(url=url, save=save, output=output)\n",
    "    images = find_images_in_md_text(data[\"content\"])\n",
    "    if len(images) == 0:\n",
    "        images = find_images_on_page(url)\n",
    "    #Extract title, timestamp\n",
    "    article = {\n",
    "        \"url\": url,\n",
    "        \"full_text\": data[\"content\"],\n",
    "        \"images\": images,\n",
    "        \"title\": data[\"title\"],\n",
    "        \"time_stamp\": data[\"publishedTime\"]\n",
    "    }\n",
    "    if save:\n",
    "        if  output is None: \n",
    "            output=str(data[\"title\"].strip().replace(\" \", \"_\")+\".json\")\n",
    "        with open(output, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(article, file, ensure_ascii=False, indent=4)\n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Life after: Escaping gang violence - Positive News', 'url': 'https://www.positive.news/lifestyle/life-after-escaping-gang-violence/', 'content': \"Cenia Elizabeth Muñoz and her husband Angel feared becoming statistics of the widespread gang violence in their native El Salvador. After fleeing to the UK – arriving just before Covid-19 did – a university bridging course and a friendly welcome has helped the family to feel safe and hopeful once again\\n\\nWhen they lived in El Salvador, Cenia Elizabeth Muñoz and her husband Angel would turn off the lights every evening and hide from the gangs that operated near their house. “We had to be so quiet. If they know you are listening, or watching them, you are in trouble,” she says. “Our lives were always at risk.”\\n\\nOne night, the couple heard gunshots and crouched under their kitchen table to shelter while awaiting the police. “A young man had been killed near our house. We could hear his family crying. We could see his dead body lying in the street.”\\n\\nAfter the couple wed in 2014, they had built their two bedroom house on land given to them by Angel’s father in San Pedro Perulapán, in the central region of Cuscatlán, hoping to have children one day. “But the gangs grew like a plague,” Muñoz recalls. “And our home was a bit isolated. We never felt safe. Gangsters hid around our house and our car. They controlled the city where we lived. You couldn’t go to the park or visit a friend in a different city that was controlled by a rival gang. They would make you show your ID card, and they might kill you.”\\n\\nSolutions every Saturday Uplift your inbox with our weekly newsletter. Positive News editors select the week’s top stories of progress, bringing you the essential briefing about what's going right. [Sign up](https://www.positive.news/letter/)\\n\\nIn the 1980s, Salvadorans who had fled civil war in their home country formed gangs in the US, especially in Los Angeles, originally to defend their communities in deprived neighbourhoods. After the war ended in 1992, the US deported thousands of gang members back to El Salvador, where they created a stranglehold of terror. In 2015, the Central American nation had the highest homicide rate in the world.\\n\\n“It’s sad, because there are many positive things about my country,” Muñoz says. “People are very friendly, the weather is almost always sunny, and there are the beaches, the rivers, the mountains. But we have not had good governance. Corruption destroys countries, and that was the case for many years. Over time, crime and poverty got worse, and the police \\\\[force\\\\] was very weak.”\\n\\nOne day, Angel was on his way to catch a bus when two gangsters demanded to see his ID, holding a gun to his head. He panicked and sprinted away until he reached a shoe shop where he hid for six hours. “He was lucky,” Muñoz adds. “We know so many people – friends, our parents’ friends – who have died at the hands of gangs.”\\n\\n> The loneliness of lockdown was the hardest part, but we knew we were safe. So, we focused on our hopes for the future\\n\\nThe couple knew they had to leave El Salvador to survive. Muñoz explains: “If we had moved \\\\[within the country\\\\], the gangsters there would know we came from a rival gang’s city.” They flew to Heathrow in early 2020 to begin their asylum claim, just before the Covid-19 pandemic set in, and were taken to Cardiff, where they were shifted between nine hotels. The loneliness of lockdown was “the hardest part”, Muñoz says, “but we knew we were safe. So, we focused on our hopes for the future.”\\n\\nAfter they were moved to a small studio flat in Reading a few months later, the family were finally able to settle into life in the UK. Eager to learn English, they took online courses, and sought advice from what they found to be a “really amazing” refugee support centre, Sanctuary in Chichester.\\n\\nThe couple were overjoyed to welcome a daughter, Grace, in 2021. “She has been the happiness in our lives,” Muñoz says, beaming. “I have really enjoyed going to playgroups with her and I have learned so much vocabulary from nursery rhymes and baby books. She inspires us to be better because I want her to feel proud about who her parents are.”\\n\\n> She inspires us to be better because I want her to feel proud about who her parents are\\n\\nIn El Salvador, Muñoz had abandoned her dreams of training as a teacher in order to earn money at a call centre. “\\\\[Then, in the UK,\\\\] I was only expecting to do something like cleaning houses – to survive,” she says. But since taking the 12-week From Adversity to University course at the University of Chichester to help people without qualifications ‘bridge the gap’ into higher education, she is now newly confident that she can achieve her ambition, and plans to teach children Spanish one day.\\n\\nLast year, the couple’s asylum claim was finally accepted and they were granted refugee status, allowing them to work. While her husband is currently a delivery driver, Muñoz is studying for an English language qualification and looking for part-time work.\\n\\n“I feel like this country has embraced me. I always find British people very friendly,” she says. “I am very grateful to \\\\[the UK\\\\]. We have opportunities that we did not have in El Salvador. We have realised that we can be more than just refugees living in this country; we can be more than immigrants. We have the opportunity to take part in this community.”\\n\\n_Photography: Alexander Thomas_\\n\\n### **Support solutions in 2024**\\n\\nPositive News is helping more people than ever to get a balanced and uplifting view of the world. While doom and gloom dominates other news outlets, our solutions journalism exists to support your wellbeing and empower you to make a difference towards a better future.\\n\\nBut our reporting has a cost and, as an independent, not-for-profit media organisation, we rely on the financial backing of our readers. If you value what we do and can afford to, please get behind our team with a regular or one-off contribution.\\n\\nGive once from just £1, or join 1,400+ others who contribute an average of £3 or more per month. You’ll be directly funding the production and sharing of our stories – helping our solutions journalism to benefit many more people.  \\n  \\nJoin our community today, and together, we’ll change the news for good.\\n\\n[**Support Positive News**](https://www.positive.news/support/)\", 'publishedTime': '2024-06-17T10:34:05+00:00'}\n"
     ]
    }
   ],
   "source": [
    "def test_jina_reader():\n",
    "    url = \"https://www.positive.news/lifestyle/life-after-escaping-gang-violence/\"\n",
    "    jina_reader(url, save=True, output=\"test.md\")\n",
    "\n",
    "test_jina_reader()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
