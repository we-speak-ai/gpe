{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping tool (NB)\n",
    "\n",
    "In this document lot of nice thing will happen :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from collections import deque\n",
    "import openai\n",
    "import MACHINE_CONFIG\n",
    "import newspaper\n",
    "import matplotlib.pyplot as plt\n",
    "from urllib.parse import urlparse\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scraper\n",
    "import os\n",
    "CONFIG_PATH = r\"C:\\Users\\andra\\OneDrive\\Dokumentumok\\Munka\\WeSpeakAi\\GoodPeople\\site-config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.site_selector(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Search for the page articles\n",
    "\n",
    "Our first task is to collect every article from the site, but only the articles that we want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles(base_url, article_rule, max_url = 1000, output_path=None, verbal=False):\n",
    "    # Base URL of the website to crawl\n",
    "    visited_urls = set()\n",
    "    articles = set()\n",
    "    urls_to_visit = deque([base_url])\n",
    "\n",
    "    # Function to check if a URL belongs to the base domain\n",
    "    def is_same_domain(url, base_domain):\n",
    "        return urlparse(url).netloc == base_domain\n",
    "\n",
    "    # Starting the crawl\n",
    "    while urls_to_visit and len(articles)<max_url:\n",
    "        current_url = urls_to_visit.popleft()\n",
    "        \n",
    "        # Skip if already visited\n",
    "        if current_url in visited_urls:\n",
    "            continue\n",
    "        \n",
    "        # Mark as visited\n",
    "        visited_urls.add(current_url)\n",
    "        if verbal: print(f\"Visiting: {current_url}\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                links = soup.find_all('a')\n",
    "                filtered_links = [s.get('href') for s in links if (not s.get('href') is None and (s.get('href').startswith('/') or s.get('href').startswith(base_url)))]\n",
    "\n",
    "                for link in filtered_links:\n",
    "                    if link:\n",
    "                        full_url = urljoin(base_url, link)\n",
    "                        if is_same_domain(full_url, urlparse(base_url).netloc):\n",
    "                            urls_to_visit.append(full_url)\n",
    "                        if full_url.startswith(str(base_url+article_rule)): \n",
    "                            articles.add(full_url)\n",
    "                            print(f\"Count: [{len(articles)} of {max_url}]\", flush=True)\n",
    "            else:\n",
    "                print(f\"Failed to fetch {current_url}. Status code: {response.status_code}\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching {current_url}: {e}\")\n",
    "    if output_path:\n",
    "        with open(output_path, 'w') as file:\n",
    "            # Iterate through the list of URLs\n",
    "            for url in articles:\n",
    "                # Write each URL to the file followed by a newline character\n",
    "                file.write(url + '\\n')\n",
    "    \n",
    "    return articles\n",
    "\n",
    "def has_common_element(list1, list2):\n",
    "    # Convert both lists to sets\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "    \n",
    "    # Check if there is any intersection between the sets\n",
    "    return not set1.isdisjoint(set2)\n",
    "\n",
    "def extract_and_count_urls(urls, lenght_of_posible_article):\n",
    "    # Dictionary to hold the occurrences of each segment\n",
    "    segment_counts = defaultdict(int)\n",
    "    \n",
    "    # List to store lists of segments for each URL\n",
    "    all_segments = []\n",
    "    articles = []\n",
    "    \n",
    "    for url in urls:\n",
    "        # Parse the URL and extract the path\n",
    "        parsed_url = urlparse(url)\n",
    "        path = parsed_url.path\n",
    "        segments = []\n",
    "        # Split the path into segments and filter out empty segments\n",
    "        for segment in path.split('/'):\n",
    "            if segment:\n",
    "                if len(segment) <= lenght_of_posible_article:\n",
    "                    segments.append(segment)\n",
    "                else:\n",
    "                    segments.append(\"possible_article\")\n",
    "                    articles.append(parsed_url)\n",
    "        # segments = [segment if len(segment) <= lenght_of_posible_article else \"possible_article\" for segment in path.split('/') if segment]\n",
    "        \n",
    "        # Append the list of segments for this URL to all_segments\n",
    "        all_segments.append(segments)\n",
    "        \n",
    "        # Count each segment\n",
    "        for segment in segments:\n",
    "            segment_counts[segment] += 1\n",
    "    \n",
    "    # Convert the dictionary to a list of lists format\n",
    "    segment_occurrences = [[segment, count] for segment, count in segment_counts.items()]\n",
    "    \n",
    "    return all_segments, segment_occurrences, articles\n",
    "\n",
    "def visualize_segment_occurrences(segment_occurrences):\n",
    "    # Sort the segments by their count in descending order\n",
    "    segment_occurrences.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Unzip the list of lists into two lists\n",
    "    segments, counts = zip(*segment_occurrences)\n",
    "    \n",
    "    # Create a bar plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(segments, counts, color='skyblue')\n",
    "    \n",
    "    # Add title and labels\n",
    "    plt.title('Occurrences of URL Segments')\n",
    "    plt.xlabel('Segments')\n",
    "    plt.ylabel('Occurrences')\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def short_urls(urls, categories, cfp=False,  min_article_lenght=12):\n",
    "    \"\"\"\n",
    "    cfp: Chategory is first in path\n",
    "    \"\"\"\n",
    "    articles = []\n",
    "    for url in urls:\n",
    "        # Parse the URL and extract the path\n",
    "        parsed_url = urlparse(url)\n",
    "        path = parsed_url.path\n",
    "        segments = [segment for segment in path.split('/') if segment]\n",
    "        has_chategory = (segments[0] in categories) if cfp and len(segments)>0 else has_common_element(categories, segments)\n",
    "        filtered_segments = [element for element in segments if element not in categories]\n",
    "        has_posible_article_lenght = any([len(element) >= min_article_lenght for element in filtered_segments if len(element) >= min_article_lenght])\n",
    "        if has_chategory and has_posible_article_lenght:\n",
    "            articles.append(parsed_url)\n",
    "    return articles\n",
    "     \n",
    "categories = [\"environment\", \"society\", \"lifestyle\", \"science\", \"economics\", \"perspective\", \"opinion\"]\n",
    "\n",
    "#get_articles(\"https://reasonstobecheerful.world/\", \"\", 100, r\"test.txt\")\n",
    "# # Example usage:\n",
    "with open(r\"test.txt\", \"r\") as file:\n",
    "    urls = file.readlines()\n",
    "\n",
    "# Extract segments and their occurrences\n",
    "segments, occurrences, articles = extract_and_count_urls(urls, 10)\n",
    "\n",
    "# articles = short_urls(urls, categories, True, 12)\n",
    "\n",
    "# Print extracted segments and occurrences\n",
    "print(\"Segments extracted from each URL:\", segments)\n",
    "print(\"Occurrences of each segment:\", occurrences)\n",
    "print(f\"Articles: {len(articles)}\")\n",
    "for article in articles:\n",
    "    print(article.netloc + article.path)\n",
    "\n",
    "# Visualize the occurrences\n",
    "visualize_segment_occurrences(occurrences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraping with JINA-AI-READER\n",
    "\n",
    "*source:* [Jina](https://jina.ai/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jina_reader(url, save=False, output=None):\n",
    "    \"\"\"\n",
    "        The response from JINA has the following structure:\n",
    "        {\n",
    "            satus: 200\n",
    "            \"data\": {\n",
    "                \"title\": The title of the article,\n",
    "                \"url\": URL of the article\n",
    "                \"content\": The article text\n",
    "                \"publishedTime\": The publiseh time in '2024-06-17T10:34:05+00:00' format\n",
    "            }        \n",
    "        }\n",
    "\n",
    "    \"\"\"\n",
    "    jina = 'https://r.jina.ai/'\n",
    "    headers = {\n",
    "    \"Authorization\": MACHINE_CONFIG.JINA_KEY,\n",
    "    \"Accept\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(jina+url, headers=headers)\n",
    "    response_text = response.text\n",
    "    # Checking the status of the request\n",
    "    if response.status_code == 200:\n",
    "        response_body = response.content.decode('utf-8')\n",
    "        response_body = json.loads(response_body)[\"data\"]\n",
    "        print(response_body)\n",
    "        title = response_body[\"title\"].strip().replace(\" \", \"_\")\n",
    "        text = response_body[\"content\"]\n",
    "    else:\n",
    "        print(f'Failed to fetch data. Status code: {response.status_code}')\n",
    "    if save:\n",
    "        if  output is None: \n",
    "            output=str(title+\".md\")\n",
    "        with open(output, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text)\n",
    "    return response_body\n",
    "    \n",
    "def find_images_in_md_text(text):\n",
    "    \"\"\"\n",
    "    # Function to find all image links in a Markdown file\n",
    "    \"\"\"\n",
    "    image_pattern = r'!\\[(.*?)\\]\\((.*?)\\)'\n",
    "    matches = re.findall(image_pattern, text)\n",
    "\n",
    "    return matches\n",
    "\n",
    "def find_images_on_page(url):\n",
    "    article = newspaper.Article(url)\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    return article.images\n",
    "\n",
    "def construct_article(url, save=False, output=None):\n",
    "    data = jina_reader(url=url, save=save, output=output)\n",
    "    images = find_images_in_md_text(data[\"content\"])\n",
    "    if len(images) == 0:\n",
    "        images = find_images_on_page(url)\n",
    "    #Extract title, timestamp\n",
    "    article = {\n",
    "        \"url\": url,\n",
    "        \"full_text\": data[\"content\"],\n",
    "        \"images\": images,\n",
    "        \"title\": data[\"title\"],\n",
    "        \"time_stamp\": data[\"publishedTime\"]\n",
    "    }\n",
    "    if save:\n",
    "        if  output is None: \n",
    "            output=str(data[\"title\"].strip().replace(\" \", \"_\")+\".json\")\n",
    "        with open(output, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(article, file, ensure_ascii=False, indent=4)\n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Life after: Escaping gang violence - Positive News', 'url': 'https://www.positive.news/lifestyle/life-after-escaping-gang-violence/', 'content': \"Cenia Elizabeth Muñoz and her husband Angel feared becoming statistics of the widespread gang violence in their native El Salvador. After fleeing to the UK – arriving just before Covid-19 did – a university bridging course and a friendly welcome has helped the family to feel safe and hopeful once again\\n\\nWhen they lived in El Salvador, Cenia Elizabeth Muñoz and her husband Angel would turn off the lights every evening and hide from the gangs that operated near their house. “We had to be so quiet. If they know you are listening, or watching them, you are in trouble,” she says. “Our lives were always at risk.”\\n\\nOne night, the couple heard gunshots and crouched under their kitchen table to shelter while awaiting the police. “A young man had been killed near our house. We could hear his family crying. We could see his dead body lying in the street.”\\n\\nAfter the couple wed in 2014, they had built their two bedroom house on land given to them by Angel’s father in San Pedro Perulapán, in the central region of Cuscatlán, hoping to have children one day. “But the gangs grew like a plague,” Muñoz recalls. “And our home was a bit isolated. We never felt safe. Gangsters hid around our house and our car. They controlled the city where we lived. You couldn’t go to the park or visit a friend in a different city that was controlled by a rival gang. They would make you show your ID card, and they might kill you.”\\n\\nSolutions every Saturday Uplift your inbox with our weekly newsletter. Positive News editors select the week’s top stories of progress, bringing you the essential briefing about what's going right. [Sign up](https://www.positive.news/letter/)\\n\\nIn the 1980s, Salvadorans who had fled civil war in their home country formed gangs in the US, especially in Los Angeles, originally to defend their communities in deprived neighbourhoods. After the war ended in 1992, the US deported thousands of gang members back to El Salvador, where they created a stranglehold of terror. In 2015, the Central American nation had the highest homicide rate in the world.\\n\\n“It’s sad, because there are many positive things about my country,” Muñoz says. “People are very friendly, the weather is almost always sunny, and there are the beaches, the rivers, the mountains. But we have not had good governance. Corruption destroys countries, and that was the case for many years. Over time, crime and poverty got worse, and the police \\\\[force\\\\] was very weak.”\\n\\nOne day, Angel was on his way to catch a bus when two gangsters demanded to see his ID, holding a gun to his head. He panicked and sprinted away until he reached a shoe shop where he hid for six hours. “He was lucky,” Muñoz adds. “We know so many people – friends, our parents’ friends – who have died at the hands of gangs.”\\n\\n> The loneliness of lockdown was the hardest part, but we knew we were safe. So, we focused on our hopes for the future\\n\\nThe couple knew they had to leave El Salvador to survive. Muñoz explains: “If we had moved \\\\[within the country\\\\], the gangsters there would know we came from a rival gang’s city.” They flew to Heathrow in early 2020 to begin their asylum claim, just before the Covid-19 pandemic set in, and were taken to Cardiff, where they were shifted between nine hotels. The loneliness of lockdown was “the hardest part”, Muñoz says, “but we knew we were safe. So, we focused on our hopes for the future.”\\n\\nAfter they were moved to a small studio flat in Reading a few months later, the family were finally able to settle into life in the UK. Eager to learn English, they took online courses, and sought advice from what they found to be a “really amazing” refugee support centre, Sanctuary in Chichester.\\n\\nThe couple were overjoyed to welcome a daughter, Grace, in 2021. “She has been the happiness in our lives,” Muñoz says, beaming. “I have really enjoyed going to playgroups with her and I have learned so much vocabulary from nursery rhymes and baby books. She inspires us to be better because I want her to feel proud about who her parents are.”\\n\\n> She inspires us to be better because I want her to feel proud about who her parents are\\n\\nIn El Salvador, Muñoz had abandoned her dreams of training as a teacher in order to earn money at a call centre. “\\\\[Then, in the UK,\\\\] I was only expecting to do something like cleaning houses – to survive,” she says. But since taking the 12-week From Adversity to University course at the University of Chichester to help people without qualifications ‘bridge the gap’ into higher education, she is now newly confident that she can achieve her ambition, and plans to teach children Spanish one day.\\n\\nLast year, the couple’s asylum claim was finally accepted and they were granted refugee status, allowing them to work. While her husband is currently a delivery driver, Muñoz is studying for an English language qualification and looking for part-time work.\\n\\n“I feel like this country has embraced me. I always find British people very friendly,” she says. “I am very grateful to \\\\[the UK\\\\]. We have opportunities that we did not have in El Salvador. We have realised that we can be more than just refugees living in this country; we can be more than immigrants. We have the opportunity to take part in this community.”\\n\\n_Photography: Alexander Thomas_\\n\\n### **Support solutions in 2024**\\n\\nPositive News is helping more people than ever to get a balanced and uplifting view of the world. While doom and gloom dominates other news outlets, our solutions journalism exists to support your wellbeing and empower you to make a difference towards a better future.\\n\\nBut our reporting has a cost and, as an independent, not-for-profit media organisation, we rely on the financial backing of our readers. If you value what we do and can afford to, please get behind our team with a regular or one-off contribution.\\n\\nGive once from just £1, or join 1,400+ others who contribute an average of £3 or more per month. You’ll be directly funding the production and sharing of our stories – helping our solutions journalism to benefit many more people.  \\n  \\nJoin our community today, and together, we’ll change the news for good.\\n\\n[**Support Positive News**](https://www.positive.news/support/)\", 'publishedTime': '2024-06-17T10:34:05+00:00'}\n"
     ]
    }
   ],
   "source": [
    "def test_jina_reader():\n",
    "    url = \"https://www.positive.news/lifestyle/life-after-escaping-gang-violence/\"\n",
    "    jina_reader(url, save=True, output=\"test.md\")\n",
    "\n",
    "test_jina_reader()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wsa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
